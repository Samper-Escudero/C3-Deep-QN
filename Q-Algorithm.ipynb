{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridBoard.py and Gridworld in the main repository have been directly copied from \n",
    "# Alexander Zai. “Deep Reinforcement Learning in Action MEAP V06”\n",
    "# whereas the text in this notebook is my own implementation of the code in the book\n",
    "# licenses are inherited from the ones used by A.Zai et al in the mentioned book\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from collections import OrderedDict\n",
    "from torch.autograd import Variable\n",
    "from Gridworld import *\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "from matplotlib import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_fn(av, tau = 1.12):\n",
    "# This function receives average rewards and outputs the softmax probabilities \n",
    "# Arguments:\n",
    "#   - av: expected averages\n",
    "#   - tau: temperature. High val exaggerates differences; low value promotes homogenity\n",
    "# Output:\n",
    "#   - Softmaxed values\n",
    "\n",
    "    softm = np.exp(av / tau) / np.sum( np.exp(av[:] / tau) )\n",
    "    return softm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QNet():\n",
    "    def __init__(self, n_in, n_out, n_hidden1, n_hidden2, action_set, gridSize = 4, gamma = 0.9):\n",
    "        self.n_in = n_in\n",
    "        self.n_out = n_out\n",
    "        self.action_set = action_set\n",
    "        self.gridSize = gridSize\n",
    "        self.newGridGame()\n",
    "        self.gamma = gamma\n",
    "       # self.one_hot_reward = np.ones(arms)\n",
    "\n",
    "        # Neural network model definition \n",
    "        self.model = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(n_in, n_hidden1)),\n",
    "            ('ReLu1', nn.ReLU(inplace = True)),\n",
    "            ('fc2', nn.Linear(n_hidden1, n_hidden2)),\n",
    "            ('ReLu2', nn.ReLU(inplace = True)),\n",
    "            ('fc3', nn.Linear(n_hidden2, n_out)),\n",
    "            ('ReLu3', nn.ReLU(inplace = True))\n",
    "        ])\n",
    "        )\n",
    "                                  \n",
    "    def newGridGame(self):\n",
    "        self.env = Gridworld(size=self.gridSize, mode='static') \n",
    "        self.refreshState()\n",
    "                                  \n",
    "    def refreshState(self):\n",
    "        # Get state with some random noise\n",
    "        state_ = self.env.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0 \n",
    "        self.state = torch.from_numpy(state_).float()\n",
    "                                  \n",
    "    def forward(self):\n",
    "        # Get reward prediction from the model\n",
    "        y_pred = self.model(self.state).squeeze() \n",
    "        # Obtain the probability distribution of the reward prediction\n",
    "        print(\"ypred: \", y_pred.data.numpy())\n",
    "        av_softmax = softmax_fn(y_pred.data.numpy(), tau=2.0)  \n",
    "        print(\"av_softmax: \", av_softmax)\n",
    "\n",
    "        av_softmax /= av_softmax.sum() \n",
    "\n",
    "        return av_softmax, y_pred\n",
    "    \n",
    "    def getQReward(self):\n",
    "        cur_reward = self.env.reward()\n",
    "        # The reward of a Q_learning algorithm takes into account propect\n",
    "        # So ,we need to get new state and the expected reward (maxQval)\n",
    "        new_state_ = self.env.board.render_np().reshape(1,64) + np.random.rand(1,64)/10.0 \n",
    "        new_state_ = torch.from_numpy(new_state_).float()\n",
    "        with torch.no_grad(): # we do not want this pred to be considered in grads\n",
    "            fut_qval = self.model(new_state_)\n",
    "        \n",
    "        # Are we in the endgame?\n",
    "        if cur_reward == -1:\n",
    "            QReward = cur_reward + self.gamma*torch.max(fut_qval)\n",
    "        else:\n",
    "            QReward = reward\n",
    "        \n",
    "        return QReward, cur_reward\n",
    "        \n",
    "    def actuate(self, av_softmax, y_pred, action_map = None):\n",
    "        # This function returns the reward to be used for backpropagation and the cur_reward obtained \n",
    "        # The backprop reward is such that only modifies the nodes related with obtaining cur_reward\n",
    "        # To do this, we copy the forward output and modify the value of the array corresponding with the \n",
    "        # choice made. The new value of this position is the cur_reward.\n",
    "        \n",
    "        # Probabilistically choose an action \n",
    "        print(\"choosing among: \", self.n_out)\n",
    "        print(\"av_softmax: \", av_softmax)\n",
    "        choice = np.random.choice(self.n_out, p=av_softmax) \n",
    "        # convert the choice to the action using the action set of the game\n",
    "        action = self.action_set[choice]\n",
    "        # Make the action\n",
    "        self.env.makeMove(action)\n",
    "        # Execute action and get the reward for it\n",
    "        Qreward, status_ = self.getQReward()\n",
    "        # Copy qvals\n",
    "        backprop_reward = y_pred.data.numpy().copy()\n",
    "        # Update val corresponding to choice so it matches the QReward\n",
    "        backprop_reward[choice] = Qreward\n",
    "                                          \n",
    "        # Return reward\n",
    "        return backprop_reward,status_\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Qmodel, loss_fn, optimizer, epochs = 5000):\n",
    "    \n",
    "    for epoch in range(epochs): # Episodic training, each epoch is a new game\n",
    "        \n",
    "        optimizer.zero_grad() # reset grads between epochs\n",
    "        # Create a new game for each epoch\n",
    "        Qmodel.newGridGame() # Also refreshes the model state\n",
    "       \n",
    "        # Track play status (has game ended?) \n",
    "        game_active = True\n",
    "        while(game_active): # while current play is going on\n",
    "            actions_prob, y_pred = Qmodel.forward() # get prob distribution and curr_reward\n",
    "            # actuate based on prob - determines explore-exploit\n",
    "            act_rewards, status = Qmodel.actuate(actions_prob, y_pred) \n",
    "            # Compute loss\n",
    "            print('in loss. y_pred {}, rewards: {}'.format(len(y_pred.data.numpy()),len(act_rewards)))\n",
    "            loss = loss_fn(y_pred.data.numpy(), act_rewards) # compute loss\n",
    "            print('epoch {}. Loss = {}'.format(epoch, loss))\n",
    "            losses.append(loss.item())\n",
    "            loss.backward() # obtain backward propagation gradients\n",
    "            optimizer.step() # apply backprop to network model\n",
    "            \n",
    "            if status == -1:\n",
    "                game_active = True\n",
    "            else:\n",
    "                game_active = False\n",
    "                \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (fc1): Linear(in_features=64, out_features=164, bias=True)\n",
      "  (ReLu1): ReLU(inplace)\n",
      "  (fc2): Linear(in_features=164, out_features=150, bias=True)\n",
      "  (ReLu2): ReLU(inplace)\n",
      "  (fc3): Linear(in_features=150, out_features=4, bias=True)\n",
      "  (ReLu3): ReLU(inplace)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set params \n",
    "action_map = {\n",
    "    0:'u',\n",
    "    1:'d',\n",
    "    2:'l',\n",
    "    3:'r'\n",
    "}\n",
    "n_out = len(action_map) # 4\n",
    "gridSize = 4\n",
    "n_in = 4*gridSize*gridSize\n",
    "learning_rate = 1e-4\n",
    "gamma = 0.9 # decay ratio\n",
    "\n",
    "# Create RL object\n",
    "Qnet = QNet(n_in, n_out, 164, 150, action_map, gridSize, gamma)\n",
    "print(Qnet.model)\n",
    "\n",
    "# Loss metric and optimization criterion for training\n",
    "loss_fn = nn.MSELoss(size_average=True)\n",
    "criterion = torch.optim.Adam(Qnet.model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ypred:  [0.         0.02122179 0.08242083 0.08360937]\n",
      "av_softmax:  [0.24417464 0.24677935 0.2544474  0.25459865]\n",
      "choosing among:  4\n",
      "av_softmax:  [0.24417464 0.24677935 0.2544474  0.25459865]\n",
      "in loss. y_pred 4, rewards: 4\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-00fbbe632d8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-65-5661d879bf92>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(Qmodel, loss_fn, optimizer, epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'in loss. y_pred {}, rewards: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_rewards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch {}. Loss = {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pytorch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2242\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2243\u001b[0m     \"\"\"\n\u001b[0;32m-> 2244\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2245\u001b[0m         warnings.warn(\"Using a target size ({}) that is different to the input size ({}). \"\n\u001b[1;32m   2246\u001b[0m                       \u001b[0;34m\"This will likely lead to incorrect results due to broadcasting. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "losses =[]\n",
    "train(Qnet, loss_fn, criterion, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Plays\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(action_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from Gridworld import Gridworld\n",
    "game = Gridworld(size=4, mode='static')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['+', '-', ' ', 'P'],\n",
       "       [' ', 'W', ' ', ' '],\n",
       "       [' ', ' ', ' ', ' '],\n",
       "       [' ', ' ', ' ', ' ']], dtype='<U2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['+', '-', ' ', ' '],\n",
       "       [' ', 'W', ' ', ' '],\n",
       "       [' ', ' ', 'P', ' '],\n",
       "       [' ', ' ', ' ', ' ']], dtype='<U2')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.makeMove('d')\n",
    "game.makeMove('d')\n",
    "game.makeMove('l')\n",
    "game.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 1, 0],\n",
       "        [0, 0, 0, 0]],\n",
       "\n",
       "       [[1, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 1, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0],\n",
       "        [0, 1, 0, 0],\n",
       "        [0, 0, 0, 0],\n",
       "        [0, 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.board.render_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
